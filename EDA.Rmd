---
title: "Your Document Title"
author: "Author Name"
date: "`r format(Sys.Date(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
    keep_md: true
  pdf_document:
    includes:
      in_header: header.tex
geometry: margin=0.87in
fontsize: 12pt
mainfont: Times New Roman
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhf{}
- \rhead{Arvind Menon, Lennart Platon Kutzschebauch, Nisrine Bachar, Jayati Sood}
- \cfoot{\thepage}
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(conflicted)
library(ggplot2)
library(GGally)
library(ggpubr)
library(tidyverse)
library(corrplot)
library(kableExtra)
library(knitr)
library(lmtest)
require(gridExtra)
library(stargazer)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(broom)

conflict_prefer("filter", "dplyr")
conflict_prefer("lag", "dplyr")
```

# Introduction

We as human beings like to divide actions into moral and amoral. Some of these amoral actions, such as murder, theft and fraud, are codified in law and we call those transgressions crimes. It is in our best interest to avoid crime happening. Therefore, we try to punish those that commit them in hopes of it being a deterrent for future repetition. However, there might be other ways in which to minimise crime. In addition, we see that crime isn't happening uniformly everywhere. This leads us to the question: what makes a crime more likely to happen in a region?\
Clay[1857], concentrating on crime in England, analysed the following characteristics: Beer-(ale)-house numbers, worship attendance and public school attendance. The argument he provides for the latter two, is both allow for the public access to Christian moral values (although he criticises that the ability to read on its own is not enough). It is to note, the "public schools" mentioned by Clay concentrated mostly on teaching read and writing and were not free but accepted anyone who could effort it. In the case of beer-houses, he argues "the temptation to animal pleasure" corrupts a person. To help his case he tries to show the positive correlation of beer-houses, the negative correlations of public school and worship attendance to crime.\
In this paper we want to verify (or deny) these effects of the aforementioned characteristics and try to predict crime rate from them using the same data set. However, before starting it is important to mention, as noticed by Clay[1857], the recorded crime rate may be inaccurate because of systematic reasons since each county handles crime and punishment differently on the executive and juridical levels. This will impact the accuracy of all possible analysis and models.

# Exploratory Data Analysis

We begin our analysis with a look at the first few lines of our dataset.

```{r}
data <- read.csv('./data/beerhall.csv')
colnames(data)<- c("County", "RegionName", "RegionCode", "Criminals_100k", "BeerAle_100k","SchoolAttendance_10k", "WorshipAttendance_2k")

# writeLines("First few data elements:\n")
kable(head(data))
writeLines("\n")
# head(data)
# writeLines("\n")

```

```{r, fig.width=5, fig.height=3}
# returns minimum, maximum, 1st quartile, median, mean and 3rd
#quartile of every variable
writeLines("Data summary:\n")
summ<-summary(data)
colnames(summ)<-c("County", "Region Name ", "Region Code", "Criminals per 100k population", "Ale/Beer Houses per 100k", "Attendants @ school per 10k", "Attendants @ public worship per 2000" )
kable(summ)
writeLines("\n")

```

The dataset contains information about 40 different counties.`County` and `RegionName` are categorical variables, and each of the 8 regions is assigned a `RegionCode`, which is a number from 1 to 8. `Criminals_100k` is the number of criminals per 100,000 inhabitants for any particular county. Similarly, `BeerAle_100k`, `SchoolAttendance_10k` and `WorshipAttendance_2k` are social indicators measured numerically as a proportion of the population. In order to better visualise the numerical data, we plot histograms of each numerical variable, with the exception of the categorical `RegionCode`.

```{r, fig.width=5, fig.height=3.5, fig.align='center'}
# plot variable-wise histograms
numeric_data <- data[, sapply(data, is.numeric)]
data %>% pivot_longer(cols = c( Criminals_100k, BeerAle_100k,SchoolAttendance_10k, WorshipAttendance_2k)) %>%
  ggplot(aes(value)) + facet_wrap(~ name, scales = "free") + geom_histogram(bins = 15)

```

The `BeerAle_100k` variable appears to be bimodal. The histogram suggests that there are two common levels of alcohol consumption within the entire population, one around the 200, and another around 400 per 100,000 population. According to the data summary, `Crimanals_100k` has a mean of 152.9 and a median of 157.5, suggesting the symmetry that is also reflected in the histogram. `SchoolAttendance_10k` is slightly left skewed, while `WorshipAttendance_2k` has a varied but loosely symmetric distribution.

The linear dependence between each pair of numeric variables is expressed in the following correlation matrix:

```{r, fig.width=6, fig.height=4, fig.align='center'}
# analyse correlation between variables
par(mfrow = c(1, 1))
M <- cor(numeric_data)
corrplot(cor(numeric_data), method = "number")
```

There appears to be negligible linear dependence between worship attendance and criminality. School attendance is slightly negatively correlated with the prevalence of crime. Criminal behaviour is positively correlated with `BeerAle_100k` with a correlation coefficient of 0.46, suggesting that counties with a more dominant culture of frequenting bars and pubs are also where more crime happens. In order to better visualise these dependencies, we regress `Criminals_100k` on each of these variables.

```{r, fig.width=5, fig.height=3.5, fig.align='center', message=FALSE, warning=FALSE}
# regression plots
fig1 <- ggplot(data, aes(BeerAle_100k, Criminals_100k)) + geom_point() +
  stat_smooth(method = "lm")
fig2 <- ggplot(data, aes(SchoolAttendance_10k, Criminals_100k)) + geom_point() +
  stat_smooth(method = "lm")
fig3 <- ggplot(data, aes(RegionCode, Criminals_100k)) + geom_point() +
  stat_smooth(method = "lm")
fig4 <- ggplot(data, aes(WorshipAttendance_2k, Criminals_100k)) + geom_point() +
  stat_smooth(method = "lm")

ggarrange(fig1, fig2, fig3, fig4)
```

There appears to be a very clear linear dependence of criminal behaviour on bar attendance, with most datapoints falling within the 95% confidence interval of the regression line. There is high variation of criminality across school attendance and worship attendance.

## Model Fitting

We use least squres to minimize the sum of squared residuals in a polynomial regression. In order to predict Criminals per 100k population, we use a polynomial regression model which uses Ale/Beer Houses per 100k, Attendants \@ school per 10k, and Attendants \@ public worship per 2000 as the features. We include the variable Attendants \@ public worship per 2000 in our analysis even if it has no correlation with the variable Criminals per 100k as no correlation doesn't mean no causation, it only suggests that there is no linear association.

Mathematically, the model can be represented as:

A polynomial regression model of degree n with three features (x1, x2, x3) can be represented as follows:

$y = \beta_0 + \beta_{11}x_1 + \beta_{12}x_1^2 + ... + \beta_{1n}x_1^n + \beta_{21}x_2 + \beta_{22}x_2^2 + ... + \beta_{2n}x_2^n + \beta_{31}x_3 + \beta_{32}x_3^2 + ... + \beta_{3n}x_3^n + \epsilon$

Where:

y is Criminals per 100k population x1, x2, x3 are the independent variables [Ale/Beer Houses per 100k, Attendants \@ school per 10k, and Attendants \@ public worship per 2000] β0, βij are the coefficients of the model where i is the feature index and j is the degree of the polynomial ε is the error term

We choose the model based on the Exploratory data analysis and the correlation between the features and the target variable. On top of the linear relation, we also add polynomial terms to capture the non-linear relation between the features and the target variable.

```{r}

# Fit a multiple linear regression model
model <- lm(Criminals_100k ~ BeerAle_100k + 
              SchoolAttendance_10k + 
              WorshipAttendance_2k, data = data)
L1O<-1:6
for (k in 1:6) {
  output<-0
  for (l in 1:40){
    traindata <- data[-l,]
    modelA <- lm(Criminals_100k ~ polym(BeerAle_100k, degree= k)+polym( SchoolAttendance_10k,degree = k) +polym (WorshipAttendance_2k, degree =  k), data = traindata)
    output <- output+(predict(modelA, newdata = data)[l]-data$Criminals_100k[l])^2
  }
  L1O[k]<-(output/40)**0.5
}

L2O<-1:6
for (k in 1:6) {
  output<-0
  for (l in 1:39){
    for (x in (l+1):40) {
      traindata <- data[-c(x,l),]
      modelA <- lm(Criminals_100k ~ polym(BeerAle_100k, degree= k)+polym( SchoolAttendance_10k,degree = k) +polym (WorshipAttendance_2k, degree =  k), data = traindata)
      output <- output+(predict(modelA, newdata = data)[x]-data$Criminals_100k[x])^2
      output <- output+(predict(modelA, newdata = data)[l]-data$Criminals_100k[l])^2
    }
  }
  L2O[k]<-(2*output/(40*39))**0.5
}
#modelF3<-lm(Criminals_100k ~ polym(BeerAle_100k, degree= 2)+polym( SchoolAttendance_10k,degree = 2) +polym (WorshipAttendance_2k, degree =  2), data=data)

modelF1<- lm(Criminals_100k ~ polym(BeerAle_100k, degree= 4)+polym( SchoolAttendance_10k,degree = 4) +polym (WorshipAttendance_2k, degree =  4), data=data)

modelF<- lm(Criminals_100k ~ polym(BeerAle_100k, degree= 3)+polym( SchoolAttendance_10k,degree = 3) +polym (WorshipAttendance_2k, degree =  3), data=data)

```

The graph below shows the errors for different polynomial degrees up to 6 using cross-validation

```{r}
# Create a first line
plot(1:6, L1O[1:6], type = "b", frame = FALSE, pch = 19, 
     col = "red", xlab = "Degrees", ylab = "Error", ylim = c(35, 100), main = "Cross validation errors per polynomial degree")
# Add a second line
lines(1:6, L2O[1:6], pch = 18, col = "blue", type = "b", lty = 2)
# Add a legend to the plot
legend("topleft", legend=c("L1O", "L2O"),
       col=c("red", "blue"), lty = 1:2, cex=0.8)


```

We choose the degree of the polynomial based on the leave-one-out and leave-two-out cross-validations results, that we run on polynomials of degrees up to 6, where we keep the two best performing models, namely the polynomial regressions of degrees 3 and 4 and compare them to the linear model (with degree 1) using Information Criteria. We get the following results :

```{r}
aic<-AIC(model, modelF, modelF1)
bic<-BIC(model, modelF, modelF1)

bic_lc<-bic[,ncol(bic)]
merged_table <- cbind(aic, bic_lc)
colnames(merged_table)<-c("DF", "AIC", "BIC")
rownames(merged_table)<-c("Linear model", "Degree 3 polynomial", "Degree 4 Polynomial")

# Print merged table
kable(merged_table, caption ="Information Criteria")

```

```{r}
lr_test<-lrtest(modelF,modelF1)

chisq<- lr_test$Chisq[2]
pr<-lr_test$`Pr(>Chisq)`[2]

lr_results<-data.frame(
  Model=c("Degree 3 polynomial", "Degree 4 polynomial"),
  Df=lr_test$`#Df`,
  LogLik=lr_test$LogLik,
  Chisq=c(" ", format(chisq, digits = 3)),
  Pr_Chisq =c(" ", sprintf("%.2e",pr))
)


lr_results$Pr_Chisq <- format(lr_results$Pr_Chisq, scientific = TRUE, digits = 2)



```



We can see that the two unrestricted models fit better the data than the simpler linear model. Using the likelihood ratio test, we compare them and we fail to reject the null hypothesis at a 5% significance level that the unrestricted linear model fits better the data and we keep the restricted model. Therefore, the polynomial model of degree 3 provides a significantly better fit to the data.

```{r}

kable(lr_results, digits = 2, caption = "Likelihood Ratio Test results")
```



Then, we run a backward selection based on the Akaike Information Criteria to select a subset of variables from the larger set of variables used in the polynomial model of degree 3. We compare all possible sub-models and pick the one that fits best the data. 

```{r}
modelF2<- step(lm(Criminals_100k ~ BeerAle_100k+ SchoolAttendance_10k+ WorshipAttendance_2k+I(BeerAle_100k^2)+ I(SchoolAttendance_10k^2)+ I(WorshipAttendance_2k^2)+I(BeerAle_100k^3)+ I(SchoolAttendance_10k^3)+ I(WorshipAttendance_2k^3), data=data),direction="backward", trace=0)



```



We get the following model :
$\hat(y) = \beta_0 + \beta_{11}x_1 + \beta_{12}x_1^2 + \beta_{21}x_2 + \beta_{22}x_2^2 + \beta_{23}x_2^3 + \beta_{31}x_3 + \beta_{32}x_3^2 + \beta_{33}x_3^3$

Where:

$\hat(y)$ is the predicted Criminals per 100k population x1, x2, x3 are Ale/Beer Houses per 100k, Attendants \@ school per 10k and Attendants \@ public worship per 2000 respectively. β0, βij are the estimates in the table below.
¨
 
```{r}

tab_model(modelF2,show.ci = FALSE, show.se = TRUE, show.stat = TRUE, show.zeroinf = TRUE, show.r2=TRUE, show.fstat=TRUE, show.aic=TRUE, show.dev = TRUE, show.loglik = TRUE, show.obs = TRUE, title="Summary statistics of the regression model", dv.labels = "Criminal per 100k", digits= 2, p.style = "numeric_stars")
```
  
  
  

## Model assessment

In this section, we will verify the conditions for our regressions model to hold, namely that our residuals have zero mean, are uncorrelated, are homoscedastic and are normally distributed.

#### Zero mean error terms

```{r}
# Get residuals from the model
residuals <- residuals(modelF2)

# Compute the mean of the residuals
mean_error <- mean(residuals)

```
We compute our model residuals mean and get : `r print(mean_error)`


#### Homoscedastic error term 

```{r}
# Assuming modelF2 is your regression model
plot(modelF2, which = 1, main = " ", sub.caption=" ")

```



We see from the plot above that our residuals are almost homoscedastic


#### Error terms are uncorrelated 

```{r}
#Autocorrelation function
acf(residuals(modelF2), main = "Series residuals")
```

```{r}
# Perform Durbin-Watson test for autocorrelation
dw<-dwtest(modelF2)
dw
```
Both the Autocorrelation function plot and the Durbin-Watson test (with autocorrelation `{r} cat(dw$r,"\n")` and p-value `r cat(dw$p,"\n")` ) suggest that we have a weak positive autocorrelation at lag 1.


#### Error terms are normally distributed



```{r}
# Q-Q plot of residuals
qqnorm(residuals(modelF2))
qqline(residuals(modelF2))
```


Given that our data set is small, we can assume from the Normal QQ plot that our residuals are approximately normally distributed

















